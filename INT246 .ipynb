{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design & Implement Neural network models (Perceprton,SVM,LVQ,SOM) on\n",
    "# Caesarian classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Delivey No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.687500</td>\n",
       "      <td>1.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.017927</td>\n",
       "      <td>0.794662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Age  Delivey No\n",
       "count  80.000000   80.000000\n",
       "mean   27.687500    1.662500\n",
       "std     5.017927    0.794662\n",
       "min    17.000000    1.000000\n",
       "25%    25.000000    1.000000\n",
       "50%    27.000000    1.000000\n",
       "75%    32.000000    2.000000\n",
       "max    40.000000    4.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(\"csv.csv\") \n",
    "#data.head()\n",
    "#data\n",
    "df = pd.DataFrame(data)\n",
    "#First\n",
    "df1 = df[df.Caesarian=='Yes']\n",
    "#Second\n",
    "df2 = df[df.Caesarian=='No']\n",
    "df.groupby('Caesarian').size()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x20b358d2d68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Caesarian')\n",
    "plt.scatter(df1['Age'],df1['Caesarian'],color = 'green', marker='+')\n",
    "plt.scatter(df2['Age'],df2['Caesarian'],color = 'blue', marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# creating the input data to our neural network\n",
    "# Are four elemtents of x1 and x2 columns\n",
    "data_input_x = np.array([[0.0, 0.0], \n",
    "                         [0.0, 1.0],\n",
    "                         [1.0, 0.0],\n",
    "                         [1.0, 1.0]])\n",
    "# creating the classification that we know to out input data ('classe' column)\n",
    "data_y = np.array([[0.0], [0.0], [0.0], [1.0]])\n",
    "\n",
    "def step_function(sum_value):\n",
    "    return tf.cast(tf.to_float(tf.math.greater_equal(sum_value, 1)), tf.float64)\n",
    "\n",
    "# Define the variables used during de processing\n",
    "# Two weights to only one neuron\n",
    "# Weights are initialized with zero\n",
    "weights = tf.Variable(tf.zeros([2,1], dtype = tf.float64))\n",
    "\n",
    "# define our outputlayer calculation\n",
    "output_layer = tf.matmul(data_input_x, weights)\n",
    "\n",
    "# define our activation function to transform the output layer values into knowed classes (0 or 1)\n",
    "predictions = step_function(output_layer)\n",
    "\n",
    "# define score function to evaluate the accuracy\n",
    "error = tf.subtract(data_y, predictions)\n",
    "\n",
    "# define delta function used to adjust the weights during the training\n",
    "delta = tf.matmul(data_input_x, error, transpose_a = True)\n",
    "learningRate = 0.1\n",
    "train = tf.assign(weights, tf.add(weights, tf.multiply(delta, learningRate)))\n",
    "\n",
    "# Create the initializer function TensorFlow Variables used during the processing\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as s:\n",
    "    s.run(init)\n",
    "    print('Output layer result: \\n', s.run(output_layer))\n",
    "    print('Prediction result: \\n', s.run(predictions))\n",
    "    print('Error result: \\n', s.run(error))\n",
    "    print('\\n')\n",
    "    for epoch in range(15):\n",
    "        train_error, _ = s.run([error, train])\n",
    "        train_error_sum = tf.reduce_sum(train_error)\n",
    "        print('Epoch: ', epoch+1, ' - Error: ', s.run(train_error_sum))\n",
    "        if train_error_sum.eval() == 0.0:\n",
    "            break; # learned and got 100% accuracy\n",
    "    print('\\nWeights to the best accuracy: \\n', s.run(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv('csv.csv')\n",
    "#print(df.head())\n",
    "y=df.iloc[:,5].values\n",
    "print(y)\n",
    "X=df.iloc[:,2:6].values\n",
    "#print(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(\n",
    "        X,y,test_size=0.2,random_state=0)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', C=1, random_state=0)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred=svm.predict(X_test)\n",
    "print('misclassified samples: %d'%(y_test!=y_pred).sum())#compute\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy:%.2f'%accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LVQ \n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    "\n",
    "# Locate the best matching unit\n",
    "def get_best_matching_unit(codebooks, test_row):\n",
    "\tdistances = list()\n",
    "\tfor codebook in codebooks:\n",
    "\t\tdist = euclidean_distance(codebook, test_row)\n",
    "\t\tdistances.append((codebook, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\treturn distances[0][0]\n",
    "\n",
    "# Make a prediction with codebook vectors\n",
    "def predict(codebooks, test_row):\n",
    "\tbmu = get_best_matching_unit(codebooks, test_row)\n",
    "\treturn bmu[-1]\n",
    "\n",
    "# Create a random codebook vector\n",
    "def random_codebook(train):\n",
    "\tn_records = len(train)\n",
    "\tn_features = len(train[0])\n",
    "\tcodebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "\treturn codebook\n",
    "\n",
    "# Train a set of codebook vectors\n",
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
    "\tfor epoch in range(epochs):\n",
    "\t\trate = lrate * (1.0-(epoch/float(epochs)))\n",
    "\t\tfor row in train:\n",
    "\t\t\tbmu = get_best_matching_unit(codebooks, row)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\terror = row[i] - bmu[i]\n",
    "\t\t\t\tif bmu[-1] == row[-1]:\n",
    "\t\t\t\t\tbmu[i] += rate * error\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbmu[i] -= rate * error\n",
    "\treturn codebooks\n",
    "\n",
    "# LVQ Algorithm\n",
    "def learning_vector_quantization(train, test, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict(codebooks, row)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)\n",
    "\n",
    "# Test LVQ on Ionosphere dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'csv.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "learn_rate = 0.3\n",
    "n_epochs = 50\n",
    "n_codebooks = 20\n",
    "scores = evaluate_algorithm(dataset, learning_vector_quantization, n_folds, n_codebooks, learn_rate, n_epochs)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def learning_rate(epoch: tf.placeholder, max_epochs: int):\n",
    "    with tf.name_scope('learning_rate'):\n",
    "        return tf.exp(-4 * epoch / max_epochs)\n",
    "\n",
    "\n",
    "def neighbourhood(r: tf.placeholder, epoch: tf.placeholder, max_epochs: int, size: int):\n",
    "    with tf.name_scope('neighbourhood'):\n",
    "        return tf.exp(\n",
    "            - (2 * r / size) ** 2\n",
    "            * (max_epochs / (max_epochs - epoch)) ** 3\n",
    "        )\n",
    "\n",
    "\n",
    "class SelfOrganisingMap:\n",
    "    def __init__(self, shape: tuple, features: int, *_,\n",
    "                 max_epochs: int = None, init: str = 'uniform', learning_rate: float = 0.1):\n",
    "        \"\"\"\n",
    "        Self-organising map using TensorFlow.\n",
    "\n",
    "        :param shape: map dimensions\n",
    "        :param features: number of input features\n",
    "        :param _: used to force calling with keyword arguments below\n",
    "        :param max_epochs: used to scale the neighbourhood and learning rate functions\n",
    "        :param init: method of weight initialisation. 'uniform' for drawing from an uniform\n",
    "            distribution between 0..1, 'normal' for drawing from X~N(0,1)\n",
    "        :param learning_rate: initial learning rate multiplier\n",
    "        \"\"\"\n",
    "        self._weights = None\n",
    "\n",
    "        self._shape = shape\n",
    "        self._features = features\n",
    "        self._neighbour_shape = (len(shape),) + tuple(1 for _ in shape) + (-1,)\n",
    "\n",
    "        self._epochs = 0\n",
    "        self._max_epochs = max_epochs\n",
    "        self._initial_lr = learning_rate\n",
    "\n",
    "        if init == 'uniform':\n",
    "            self._initialiser = tf.random_uniform_initializer\n",
    "        elif init == 'normal':\n",
    "            self._initialiser = tf.random_normal_initializer\n",
    "        else:\n",
    "            raise AssertionError('Unknown weights initialiser type \"%s\"!' % init)\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        if self._weights is None:\n",
    "            raise ValueError('Map not fitted!')\n",
    "        return self._weights\n",
    "\n",
    "    @property\n",
    "    def initialiser(self):\n",
    "        if self._weights is None:\n",
    "            return self._initialiser\n",
    "        else:\n",
    "            return tf.convert_to_tensor(self._weights)\n",
    "\n",
    "    @property\n",
    "    def shape(self): return self._shape\n",
    "\n",
    "    @property\n",
    "    def n_nodes(self): return int(np.prod(self.shape))\n",
    "\n",
    "    @property\n",
    "    def features(self): return self._features\n",
    "\n",
    "    @property\n",
    "    def epochs(self): return self._epochs\n",
    "\n",
    "    @property\n",
    "    def max_epochs(self): return self._max_epochs\n",
    "\n",
    "    def project(self, data: np.ndarray) -> np.array:\n",
    "        \"\"\"\n",
    "        Project data onto the map. NumPy implementation for simplicity.\n",
    "\n",
    "        :param data: samples\n",
    "        :return: node indices\n",
    "        \"\"\"\n",
    "        diff = self.weights - data\n",
    "        dist = np.sum(diff ** 2, axis=-1, keepdims=True)\n",
    "        return np.array(np.unravel_index(\n",
    "            np.argmin(dist.reshape((-1, data.shape[0])), axis=0), self.shape\n",
    "        ))\n",
    "\n",
    "    def train(self, x: np.ndarray, epochs: int, batch_size: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Create training graph and train SOM.\n",
    "\n",
    "        :param x: training data\n",
    "        :param epochs: number of epochs to train\n",
    "        :param batch_size: number of training examples per step\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        graph = tf.Graph()\n",
    "        sess = tf.Session(graph=graph)\n",
    "\n",
    "        x = x.astype(np.float64)\n",
    "\n",
    "        if x.shape[0] % batch_size != 0:\n",
    "            raise ValueError('Bad batch_size, last batch would be incomplete!')\n",
    "\n",
    "        # Construct graph\n",
    "        with graph.as_default():\n",
    "            indices = tf.convert_to_tensor(np.expand_dims(\n",
    "                np.indices(self.shape, dtype=np.float64), axis=-1\n",
    "            ))\n",
    "            weights = tf.get_variable(\n",
    "                'weights', (*self.shape, 1, self.features), initializer=self.initialiser, dtype=tf.float64\n",
    "            )\n",
    "\n",
    "            with tf.name_scope('data'):\n",
    "                data = tf.data.Dataset.from_tensor_slices(x)\n",
    "                data = data.shuffle(buffer_size=10000).repeat(epochs)\n",
    "                data = data.batch(batch_size, drop_remainder=True)\n",
    "                data = data.make_one_shot_iterator().get_next()\n",
    "\n",
    "            with tf.name_scope('winner'):\n",
    "                diff = weights - data\n",
    "                dist = tf.reduce_sum(diff ** 2, axis=-1, keepdims=True)\n",
    "                w_ix = tf.argmin(tf.reshape(dist, (self.n_nodes, data.shape[0])), axis=0)\n",
    "                winner_op = tf.convert_to_tensor(tf.unravel_index(w_ix, self.shape))\n",
    "\n",
    "            with tf.name_scope('update'):\n",
    "                curr_epoch = tf.placeholder(dtype=tf.int64, shape=())\n",
    "\n",
    "                idx_diff = indices - tf.reshape(tf.cast(\n",
    "                    winner_op, dtype=tf.float64\n",
    "                ), shape=self._neighbour_shape)\n",
    "                idx_dist = tf.norm(idx_diff, axis=0)\n",
    "\n",
    "                l_rate = learning_rate(curr_epoch, self.max_epochs)\n",
    "                n_hood = neighbourhood(\n",
    "                    idx_dist, curr_epoch, self.max_epochs, max(self.shape)\n",
    "                )\n",
    "\n",
    "                update = diff * l_rate * tf.expand_dims(n_hood, axis=-1)\n",
    "                update_op = weights.assign(\n",
    "                    weights - self._initial_lr * tf.reduce_sum(update, axis=-2, keepdims=True)\n",
    "                )\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "\n",
    "        # Initialise all variables\n",
    "        sess.run(init)\n",
    "\n",
    "        batches = int(np.ceil(x.shape[0] // batch_size))\n",
    "        for i in tqdm(range(epochs)):\n",
    "            for b in range(batches):\n",
    "                sess.run(update_op, feed_dict={\n",
    "                    curr_epoch: self.epochs + i\n",
    "                })\n",
    "\n",
    "        self._weights = sess.run(weights)\n",
    "        self._epochs += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "a, _ = load_iris(True)\n",
    "a, _ = shuffle(a, _)\n",
    "a = RobustScaler().fit_transform(a)\n",
    "\n",
    "epochs = 100\n",
    "som = SelfOrganisingMap((100, 100), 4, max_epochs=epochs, init='normal')\n",
    "som.train(a, epochs, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
